{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23.2 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torchvision in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (0.18.0)\n",
      "Requirement already satisfied: numpy in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.3.0 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torchvision) (2.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: filelock in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torch==2.3.0->torchvision) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torch==2.3.0->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torch==2.3.0->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torch==2.3.0->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torch==2.3.0->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torch==2.3.0->torchvision) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.2-cp311-cp311-macosx_12_0_arm64.whl (10.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.0-cp311-cp311-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.3/30.3 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.4.2 scipy-1.13.0 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting lpips\n",
      "  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m841.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=0.4.0 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from lpips) (2.3.0)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from lpips) (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.14.3 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from lpips) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from lpips) (1.13.0)\n",
      "Requirement already satisfied: tqdm>=4.28.1 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from lpips) (4.66.4)\n",
      "Requirement already satisfied: filelock in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (2024.3.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from torchvision>=0.2.1->lpips) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from jinja2->torch>=0.4.0->lpips) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/readymadelife/Library/Caches/pypoetry/virtualenvs/dacon-bird-classification-RncD4GgV-py3.11/lib/python3.11/site-packages (from sympy->torch>=0.4.0->lpips) (1.3.0)\n",
      "Installing collected packages: lpips\n",
      "Successfully installed lpips-0.1.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install scikit-learn\n",
    "!pip install lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import timm\n",
    "from torch import nn\n",
    "\n",
    "class MaxViTModel(nn.Module):\n",
    "    def __init__(self, y_dim: int = 10):\n",
    "        super(MaxViTModel, self).__init__()\n",
    "        self.model = timm.create_model(\n",
    "            \"maxvit_xlarge_tf_384\",\n",
    "            pretrained=True,\n",
    "            num_classes=y_dim,\n",
    "        )\n",
    "        # self.model.head = nn.Linear(self.model.head.in_features, y_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Swinv2Model(nn.Module):\n",
    "    def __init__(self, y_dim: int = 10):\n",
    "        super(Swinv2Model, self).__init__()\n",
    "        self.model = timm.create_model(\n",
    "            \"swinv2_large_window12to16_192to256\",\n",
    "            pretrained=True,\n",
    "            num_classes=y_dim,\n",
    "        )\n",
    "        # self.model.head = nn.Linear(self.model.head.in_features, y_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class EvaV2Model(nn.Module):\n",
    "    def __init__(self, y_dim: int = 10):\n",
    "        super(EvaV2Model, self).__init__()\n",
    "        self.model = timm.create_model(\n",
    "            \"eva02_large_patch14_224.mim_in22k\",\n",
    "            pretrained=True,\n",
    "            num_classes=y_dim,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class EdsrModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels,\n",
    "        num_features,\n",
    "        num_res_blocks,\n",
    "        res_scale=0.1,\n",
    "        upscale_factor=2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(EdsrModel, self).__init__()\n",
    "        self.input_conv = nn.Conv2d(\n",
    "            num_channels, num_features, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[EdsrResidualBlock(num_features, res_scale) for _ in range(num_res_blocks)]\n",
    "        )\n",
    "        self.upsample = EdsrUpsampleBlock(num_features, upscale_factor)\n",
    "        self.output_conv = nn.Conv2d(\n",
    "            num_features, num_channels, kernel_size=3, padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.input_conv(x)\n",
    "        out = self.res_blocks(residual)\n",
    "        out = self.upsample(out + residual)\n",
    "        out = self.output_conv(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class EdsrResidualBlock(nn.Module):\n",
    "    def __init__(self, num_features, res_scale):\n",
    "        super(EdsrResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_features, num_features, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(num_features, num_features, kernel_size=3, padding=1)\n",
    "        self.res_scale = res_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.conv1(x)\n",
    "        residual = self.relu(residual)\n",
    "        residual = self.conv2(residual)\n",
    "        return x + residual * self.res_scale\n",
    "\n",
    "\n",
    "class EdsrUpsampleBlock(nn.Module):\n",
    "    def __init__(self, num_features, upscale_factor):\n",
    "        super(EdsrUpsampleBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            num_features, num_features * (4), kernel_size=3, padding=1\n",
    "        )\n",
    "        self.pixel_shuffle1 = nn.PixelShuffle(2)\n",
    "\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            num_features, num_features * (4), kernel_size=3, padding=1\n",
    "        )\n",
    "        self.pixel_shuffle2 = nn.PixelShuffle(2)\n",
    "\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pixel_shuffle1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pixel_shuffle2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "import os, random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_path_list: list,\n",
    "        label_list: list,\n",
    "        img_dir: str,\n",
    "        device: str = \"cpu\",\n",
    "        transform=None,\n",
    "        transform_model=None,\n",
    "    ):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "        self.device = device\n",
    "        self.transform_model = (\n",
    "            transform_model.to(device) if transform_model else transform_model\n",
    "        )\n",
    "        if self.transform_model:\n",
    "            self.transform_model.eval()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_path_list[idx])\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transform_model:\n",
    "            image = TF.to_tensor(image)\n",
    "            image = TF.normalize(\n",
    "                image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                image = image.to(self.device)\n",
    "                image = self.transform_model(image)\n",
    "                image = image.to(\"cpu\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.label_list is not None and len(self.label_list) > 0:\n",
    "            y = self.label_list[idx]\n",
    "            return image, y\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "\n",
    "class UpscaleDataset(Dataset):\n",
    "    def __init__(self, lr_path_list, hr_path_list, img_dir, mode=\"train\", **kwrags):\n",
    "        self.img_dir = img_dir\n",
    "        self.mode = mode\n",
    "        self.lr_path_list = lr_path_list\n",
    "        self.hr_path_list = hr_path_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_path_list)\n",
    "\n",
    "    def transform_img(self, lr_img, hr_img):\n",
    "        if self.mode == \"eval\" or hr_img == None:\n",
    "            lr_img_tf = TF.to_tensor(lr_img)\n",
    "            lr_img_tf = TF.normalize(\n",
    "                lr_img_tf, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "            return lr_img_tf\n",
    "        if self.mode == \"train\":\n",
    "            lr_img_tf = lr_img\n",
    "            hr_img_tf = hr_img\n",
    "            if random.random() > 0.5:\n",
    "                lr_img_tf = TF.hflip(lr_img_tf)\n",
    "                hr_img_tf = TF.hflip(hr_img_tf)\n",
    "\n",
    "            random_rotate_angle = random.randint(0, 360)\n",
    "            lr_img_tf = TF.rotate(lr_img_tf, random_rotate_angle)\n",
    "            hr_img_tf = TF.rotate(hr_img_tf, random_rotate_angle)\n",
    "\n",
    "            lr_img_tf = TF.to_tensor(lr_img_tf)\n",
    "            hr_img_tf = TF.to_tensor(hr_img_tf)\n",
    "\n",
    "            lr_img_tf = TF.normalize(\n",
    "                lr_img_tf, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "            return lr_img_tf, hr_img_tf\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_img_path = os.path.join(self.img_dir, self.lr_path_list[idx])\n",
    "        lr_img = Image.open(lr_img_path)\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            hr_img_path = os.path.join(self.img_dir, self.hr_path_list[idx])\n",
    "            hr_img = Image.open(hr_img_path)\n",
    "            lr_img_tf, hr_img_tf = self.transform_img(lr_img, hr_img)\n",
    "\n",
    "            return lr_img_tf, hr_img_tf\n",
    "\n",
    "        lr_img_tf = self.transform_img(lr_img)\n",
    "        return lr_img_tf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# earlystopping\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, name=\"model\"):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.val_loss = float(\"inf\")\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping couter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f\"Validation loss decreased ({self.val_loss:.6f} --> {val_loss:.6f}).  Saving model ...\"\n",
    "            )\n",
    "        torch.save(model.state_dict(), f\"checkpoint_{self.name}.pt\")\n",
    "        self.val_loss = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upscale train\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import lpips\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# split train, validate, test dataset\n",
    "def split_train_val_test(data, y_label):\n",
    "    train, test, _, _ = train_test_split(data, data[y_label], test_size=0.01)\n",
    "\n",
    "    train, val, _, _ = train_test_split(train, train[y_label], test_size=0.25)\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "# label encodeing\n",
    "def encode_lable(pkl_path, train, val, test):\n",
    "    if not os.path.exists(pkl_path):\n",
    "        label_encoding = preprocessing.LabelEncoder()\n",
    "        train[\"label\"] = label_encoding.fit_transform(train[\"label\"])\n",
    "        val[\"label\"] = label_encoding.transform(val[\"label\"])\n",
    "        test[\"label\"] = label_encoding.transform(test[\"label\"])\n",
    "        label_count = len(label_encoding.classes_)\n",
    "        with open(pkl_path, \"wb\") as f:\n",
    "            pickle.dump(label_encoding, f)\n",
    "\n",
    "    else:\n",
    "        with open(pkl_path, \"rb\") as f:\n",
    "            label_encoding = pickle.load(f)\n",
    "        train[\"label\"] = label_encoding.transform(train[\"label\"])\n",
    "        val[\"label\"] = label_encoding.transform(val[\"label\"])\n",
    "        test[\"label\"] = label_encoding.transform(test[\"label\"])\n",
    "        label_count = len(label_encoding.classes_)\n",
    "    return train, val, test, label_count\n",
    "\n",
    "\n",
    "# dataloader\n",
    "\n",
    "\n",
    "def convert_dataset(train, val, test, transform):\n",
    "    train_dataset = UpscaleDataset(\n",
    "        train[\"img_path\"].values,\n",
    "        train[\"upscale_img_path\"].values,\n",
    "        \"/data/\",\n",
    "        transform=transform,\n",
    "    )\n",
    "    val_dataset = UpscaleDataset(\n",
    "        val[\"img_path\"].values,\n",
    "        val[\"upscale_img_path\"].values,\n",
    "        \"/data/\",\n",
    "        transform=transform,\n",
    "    )\n",
    "    test_dataset = UpscaleDataset(\n",
    "        test[\"img_path\"].values,\n",
    "        test[\"upscale_img_path\"].values,\n",
    "        \"/data/\",\n",
    "        transform=transform,\n",
    "    )\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "def get_dataloader(train_dataset, val_dataset, test_dataset, batch_size, shuffle):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "def calculate_validation_loss_and_lpips(model, val_dataloader, criterion, device):\n",
    "    lpips_net = lpips.LPIPS(net=\"vgg\").to(device)\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    lpips_values = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            total_count += images.size(0)\n",
    "\n",
    "            # prediction\n",
    "            perceptual_distance = lpips_net(outputs, labels)\n",
    "            lpips_values.append(perceptual_distance.mean().item())\n",
    "\n",
    "    average_loss = total_loss / total_count\n",
    "    average_lpips = sum(lpips_values) / len(lpips_values)\n",
    "    return average_loss, average_lpips\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    scheduler,\n",
    "    verbose=True,\n",
    "):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"-------------------Epoch {epoch} start-------------------\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f\"Epoch {epoch+1}: current learning rate = {current_lr}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if verbose and (step % 50 == 0):\n",
    "                print(f\"Epoch {epoch + 1}, Batch {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        val_loss, lpips = calculate_validation_loss_and_lpips(\n",
    "            model, val_dataloader=val_dataloader, criterion=criterion, device=device\n",
    "        )\n",
    "        early_stopping(val_loss=val_loss, model=model)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, lpips: {lpips:.4f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "DATA_PATH = \"/data/train.csv\"\n",
    "LABEL_ENCODER_PKL_PATH = \"project/results/label_encoder/label_encoder.pkl\"\n",
    "IS_LABEL_ENCODER = False\n",
    "NAME = \"image_upscale_32_256_doubleUpsample\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 200\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TRANSFORM = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "LEARNING_RATE = 0.00001\n",
    "\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "\n",
    "train, val, test = split_train_val_test(data, \"upscale_img_path\")\n",
    "\n",
    "if IS_LABEL_ENCODER:\n",
    "    train, val, test, label_count = encode_lable(\n",
    "        LABEL_ENCODER_PKL_PATH, train, val, test\n",
    "    )\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = convert_dataset(\n",
    "    train, val, test, transform=TRANSFORM\n",
    ")\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = get_dataloader(\n",
    "    train_dataset, val_dataset, test_dataset, BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "# train\n",
    "upscale_model = EdsrModel(\n",
    "    num_channels=3, num_features=256, num_res_blocks=32, upscale_factor=4\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "criterion = nn.L1Loss().to(DEVICE)\n",
    "optimizer = optim.Adam(upscale_model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, \"min\", factor=0.1, patience=2)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True, name=NAME)\n",
    "\n",
    "train_model(\n",
    "    upscale_model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    NUM_EPOCHS,\n",
    "    DEVICE,\n",
    "    scheduler,\n",
    "    verbose=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model train\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.nn.functional import interpolate\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# split train, validate, test dataset\n",
    "def split_train_val_test(data, y_label):\n",
    "    train, test, _, _ = train_test_split(\n",
    "        data, data[y_label], test_size=0.01, stratify=data[y_label]\n",
    "    )\n",
    "\n",
    "    train, val, _, _ = train_test_split(\n",
    "        train, train[y_label], test_size=0.25, stratify=train[y_label]\n",
    "    )\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "# label encodeing\n",
    "def encode_lable(pkl_path, train, val, test):\n",
    "    if not os.path.exists(pkl_path):\n",
    "        label_encoding = preprocessing.LabelEncoder()\n",
    "        train[\"label\"] = label_encoding.fit_transform(train[\"label\"])\n",
    "        val[\"label\"] = label_encoding.transform(val[\"label\"])\n",
    "        test[\"label\"] = label_encoding.transform(test[\"label\"])\n",
    "        label_count = len(label_encoding.classes_)\n",
    "        with open(pkl_path, \"wb\") as f:\n",
    "            pickle.dump(label_encoding, f)\n",
    "\n",
    "    else:\n",
    "        with open(pkl_path, \"rb\") as f:\n",
    "            label_encoding = pickle.load(f)\n",
    "        train[\"label\"] = label_encoding.transform(train[\"label\"])\n",
    "        val[\"label\"] = label_encoding.transform(val[\"label\"])\n",
    "        test[\"label\"] = label_encoding.transform(test[\"label\"])\n",
    "        label_count = len(label_encoding.classes_)\n",
    "    return train, val, test, label_count\n",
    "\n",
    "\n",
    "# dataloader\n",
    "\n",
    "\n",
    "def convert_dataset(train, val, test, transform, transform_model, device):\n",
    "    train_dataset = CustomDataset(\n",
    "        train[\"img_path\"].values,\n",
    "        train[\"label\"].values,\n",
    "        \"/data/\",\n",
    "        transform=transform,\n",
    "        transform_model=transform_model,\n",
    "        device=device,\n",
    "    )\n",
    "    val_dataset = CustomDataset(\n",
    "        val[\"img_path\"].values,\n",
    "        val[\"label\"].values,\n",
    "        \"/data/\",\n",
    "        transform=transform,\n",
    "        transform_model=transform_model,\n",
    "        device=device,\n",
    "    )\n",
    "    test_dataset = CustomDataset(\n",
    "        test[\"img_path\"].values,\n",
    "        test[\"label\"].values,\n",
    "        \"/data/\",\n",
    "        transform=transform,\n",
    "    )\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "def get_dataloader(train_dataset, val_dataset, test_dataset, batch_size, shuffle):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "# train\n",
    "def calculate_validation_loss_and_f1(model, val_dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            total_count += images.size(0)\n",
    "\n",
    "            # prediction\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    average_loss = total_loss / total_count\n",
    "    macro_f1 = f1_score(all_labels, all_predictions, average=\"macro\")\n",
    "    return average_loss, macro_f1\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    scheduler,\n",
    "    verbose=True,\n",
    "):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"-------------------Epoch {epoch} start-------------------\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f\"Epoch {epoch+1}: current learning rate = {current_lr}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if verbose and (step % 50 == 0):\n",
    "                print(f\"Epoch {epoch + 1}, Batch {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        val_loss, macro_f1 = calculate_validation_loss_and_f1(\n",
    "            model, val_dataloader=val_dataloader, criterion=criterion, device=device\n",
    "        )\n",
    "        early_stopping(val_loss=val_loss, model=model)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Macro f1: {macro_f1:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eva02 upscale_model\n",
    "\n",
    "\n",
    "DATA_PATH = \"/data/train.csv\"\n",
    "LABEL_ENCODER_PKL_PATH = \"project/results/label_encoder/label_encoder.pkl\"\n",
    "IS_LABEL_ENCODER = True\n",
    "NAME = \"checkpoint_evav2_224_large_patch14_transform_upscaleModel_sgd\"\n",
    "BATCH_SIZE = 2\n",
    "NUM_EPOCHS = 1000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TRANSFORM = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.Resize((224, 224)),\n",
    "        # transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "\n",
    "train, val, test = split_train_val_test(data, \"label\")\n",
    "\n",
    "if IS_LABEL_ENCODER:\n",
    "    train, val, test, label_count = encode_lable(\n",
    "        LABEL_ENCODER_PKL_PATH, train, val, test\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = convert_dataset(\n",
    "    train,\n",
    "    val,\n",
    "    test,\n",
    "    transform=TRANSFORM,\n",
    "    transform_model=upscale_model,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = get_dataloader(\n",
    "    train_dataset, val_dataset, test_dataset, BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "# train\n",
    "first_model = EvaV2Model(y_dim=label_count).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.SGD(first_model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, \"min\", factor=0.1, patience=2)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True, name=NAME)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_model(\n",
    "    first_model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    NUM_EPOCHS,\n",
    "    DEVICE,\n",
    "    scheduler,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second model\n",
    "\n",
    "DATA_PATH = \"/data/train.csv\"\n",
    "LABEL_ENCODER_PKL_PATH = \"project/results/label_encoder/label_encoder.pkl\"\n",
    "IS_LABEL_ENCODER = True\n",
    "NAME = \"checkpoint_evav2_224_large_patch14_transform_bicubic_sgd\"\n",
    "BATCH_SIZE = 2\n",
    "NUM_EPOCHS = 1000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TRANSFORM = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.Resize((224, 224), interpolation=Image.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "\n",
    "train, val, test = split_train_val_test(data, \"label\")\n",
    "\n",
    "if IS_LABEL_ENCODER:\n",
    "    train, val, test, label_count = encode_lable(\n",
    "        LABEL_ENCODER_PKL_PATH, train, val, test\n",
    "    )\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = convert_dataset(\n",
    "    train,\n",
    "    val,\n",
    "    test,\n",
    "    transform=TRANSFORM,\n",
    "    transform_model=None,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = get_dataloader(\n",
    "    train_dataset, val_dataset, test_dataset, BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "# train\n",
    "second_model = EvaV2Model(y_dim=label_count).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.SGD(second_model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, \"min\", factor=0.1, patience=2)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True, name=NAME)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_model(\n",
    "    second_model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    NUM_EPOCHS,\n",
    "    DEVICE,\n",
    "    scheduler,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third_model\n",
    "DATA_PATH = \"/data/train.csv\"\n",
    "LABEL_ENCODER_PKL_PATH = \"project/results/label_encoder/label_encoder.pkl\"\n",
    "IS_LABEL_ENCODER = True\n",
    "NAME = \"checkpoint_swinv2_256_large_transform_upscaleModel_sgd\"\n",
    "BATCH_SIZE = 2\n",
    "NUM_EPOCHS = 1000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TRANSFORM = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.Resize((256, 256)),\n",
    "        # transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "\n",
    "train, val, test = split_train_val_test(data, \"label\")\n",
    "\n",
    "if IS_LABEL_ENCODER:\n",
    "    train, val, test, label_count = encode_lable(\n",
    "        LABEL_ENCODER_PKL_PATH, train, val, test\n",
    "    )\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = convert_dataset(\n",
    "    train,\n",
    "    val,\n",
    "    test,\n",
    "    transform=TRANSFORM,\n",
    "    transform_model=upscale_model,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = get_dataloader(\n",
    "    train_dataset, val_dataset, test_dataset, BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "# train\n",
    "third_model = Swinv2Model(y_dim=label_count).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.SGD(third_model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, \"min\", factor=0.1, patience=2)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True, name=NAME)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_model(\n",
    "    third_model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    NUM_EPOCHS,\n",
    "    DEVICE,\n",
    "    scheduler,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fourth_model\n",
    "DATA_PATH = \"/data/train.csv\"\n",
    "LABEL_ENCODER_PKL_PATH = \"project/results/label_encoder/label_encoder.pkl\"\n",
    "IS_LABEL_ENCODER = True\n",
    "NAME = \"checkpoint_maxvit_384_xlarge_transform_upscaleModel_sgd\"\n",
    "BATCH_SIZE = 2\n",
    "NUM_EPOCHS = 1000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TRANSFORM = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.Resize((384, 384)),\n",
    "        # transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "\n",
    "train, val, test = split_train_val_test(data, \"label\")\n",
    "\n",
    "if IS_LABEL_ENCODER:\n",
    "    train, val, test, label_count = encode_lable(\n",
    "        LABEL_ENCODER_PKL_PATH, train, val, test\n",
    "    )\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = convert_dataset(\n",
    "    train,\n",
    "    val,\n",
    "    test,\n",
    "    transform=TRANSFORM,\n",
    "    transform_model=upscale_model,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = get_dataloader(\n",
    "    train_dataset, val_dataset, test_dataset, BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "# train\n",
    "fourth_model = MaxViTModel(y_dim=label_count).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.SGD(fourth_model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, \"min\", factor=0.1, patience=2)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True, name=NAME)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_model(\n",
    "    fourth_model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    NUM_EPOCHS,\n",
    "    DEVICE,\n",
    "    scheduler,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "INFERENCE_DATA = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "with open(\"project/results/label_encoder/label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "label_count = len(label_encoder.classes_)\n",
    "\n",
    "TRANSFORMS = [\n",
    "    transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            # transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224), interpolation=Image.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((256, 256)),\n",
    "            # transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "\n",
    "    transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((384, 384)),\n",
    "            # transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "]\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "test_datasets = [\n",
    "    CustomDataset(\n",
    "        INFERENCE_DATA[\"img_path\"].values,\n",
    "        None,\n",
    "        \"/data/\",\n",
    "        transform=TRANSFORMS[0],\n",
    "        device=DEVICE,\n",
    "        transform_model=upscale_model,\n",
    "    ),\n",
    "    CustomDataset(\n",
    "        INFERENCE_DATA[\"img_path\"].values,\n",
    "        None,\n",
    "        \"/data/\",\n",
    "        transform=TRANSFORMS[1],\n",
    "        device=DEVICE,\n",
    "        transform_model=None,\n",
    "    ),\n",
    "    CustomDataset(\n",
    "        INFERENCE_DATA[\"img_path\"].values,\n",
    "        None,\n",
    "        \"/data/\",\n",
    "        transform=TRANSFORMS[2],\n",
    "        device=DEVICE,\n",
    "        transform_model=upscale_model,\n",
    "    ),\n",
    "    CustomDataset(\n",
    "        INFERENCE_DATA[\"img_path\"].values,\n",
    "        None,\n",
    "        \"/data/\",\n",
    "        transform=TRANSFORMS[3],\n",
    "        device=DEVICE,\n",
    "        transform_model=upscale_model,\n",
    "    ),\n",
    "]\n",
    "\n",
    "test_dataloaders = [\n",
    "    DataLoader(test_datasets[0], batch_size=1, shuffle=False),\n",
    "    DataLoader(test_datasets[1], batch_size=1, shuffle=False),\n",
    "    DataLoader(test_datasets[2], batch_size=1, shuffle=False),\n",
    "    DataLoader(test_datasets[3], batch_size=1, shuffle=False),\n",
    "]\n",
    "\n",
    "models = [\n",
    "    [\n",
    "        first_model\n",
    "    ],\n",
    "    [\n",
    "        second_model\n",
    "    ],\n",
    "    [\n",
    "        third_model\n",
    "    ],\n",
    "    [\n",
    "        fourth_model\n",
    "    ],\n",
    "]\n",
    "\n",
    "\n",
    "result = defaultdict(list)\n",
    "for idx, model in enumerate(models):\n",
    "    current_model = model[0]\n",
    "\n",
    "    current_model.eval()\n",
    "\n",
    "    print(f\"---------model {idx+1} inference start---------\")\n",
    "    with torch.no_grad():\n",
    "        for step, images in enumerate(test_dataloaders[idx]):\n",
    "            images = images.to(DEVICE)\n",
    "            outputs = current_model(images)\n",
    "            result[idx].append(outputs)\n",
    "\n",
    "    print(f\"---------model {idx+1} inference end---------\")\n",
    "\n",
    "\n",
    "final_predictions = []\n",
    "for batch_idx in range(\n",
    "    len(test_dataloaders[0])\n",
    "):  \n",
    "    batch_predictions = torch.stack(\n",
    "        [result[model_idx][batch_idx] for model_idx in range(len(models))]\n",
    "    )\n",
    "    mean_predictions = torch.mean(batch_predictions, dim=0)\n",
    "\n",
    "    _, predicted_labels = torch.max(mean_predictions, dim=1)\n",
    "    final_predictions.extend(predicted_labels.tolist())\n",
    "\n",
    "result = label_encoder.inverse_transform(final_predictions)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission[\"id\"] = INFERENCE_DATA[\"id\"].values\n",
    "submission[\"label\"] = result\n",
    "\n",
    "submission.to_csv(f\"submission_{datetime.today()}.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon-bird-classification-RncD4GgV-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
